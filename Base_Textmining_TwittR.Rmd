---
title: "StartCo Project"
output: html_notebook
---


https://github.com/datadolphyn/R/blob/master/SentimentAnalysis.R
https://github.com/srdas/user2016/blob/master/TextMining_UseR2016.Rmd


```{r set up}
if(!require("pacman")) install.packages("pacman")
pacman::p_load(twitteR,sentimentr,plyr,ggplot2,tm,wordcloud,textir,qdap,maptpx,RCurl,ROAuth,stringr,RJSONIO,bitops,base64enc,streamR,reshape)
library(twitteR);library(sentimentr);library(plyr);library(ggplot2);library(RColorBrewer);library(tm);library(wordcloud);library(textir);library(qdap);library(maptpx);library(RCurl);library(ROAuth);library(stringr);library(dplyr);library(RJSONIO);library(bitops);library(base64enc);library(reshape)
```

```{r Ajax Twitter APi keys}
setupCrawler <- function() {
  # Load the packages
  library(bitops)
  library(RCurl)
  library(RJSONIO)
  library(ROAuth)
  library(twitteR)
  library(stringr)
  
  # Provide Tokens (apps.twitter.com)
  api_key <- c("BaXijPbyZTBtmjKsXVDPwTPCc")
  api_secret <- c("qwuO61IXqt7C3HuGs9USpVm5lsMF3HbHxhdViw8cUmCftzTI2i") 
  token <- c("80344503-5mEqgYjpElSZ9euB0CsrrnIfgiy2AG9O5CtWEzlOE") 
  token_secret <- c("s0kTmeiautNWCOfkDPmkJ4gTsUafzoz6iNtIEijWW4mN8")
  
  # Create Twitter Connection
  # [1] "Using direct authentication"
  # vUse a local file to cache OAuth access credentials between R sessions?
  # 1: Yes
  # 2: No
  origop <- options("httr_oauth_cache")
  options(httr_oauth_cache = TRUE)
  setup_twitter_oauth(api_key, api_secret, token, token_secret)
  options(httr_oauth_cache = origop)
}  

#download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem")
#setup_twitter_oauth(api_key,api_secret,token,token_secret)

#cred <- OAuthFactory$new(consumerKey=api_key,
#                         consumerSecret=api_secret,
#                         requestURL='https://api.twitter.com/oauth/request_token',
#                         accessURL ='	https://api.twitter.com/oauth/access_token',
#                         authURL= 'https://api.twitter.com/oauth/authorize'
#                         )

#source("setupCrawler.R") #include external script source
setupCrawler()
```

```{r sentiment function}
#this function cleans the tweets and returns the merged data frame
score.sentiment = function(sentences, huliu.positive, huliu.negative, .progress='none')
{
	require(plyr)
	require(stringr)
	list=lapply(sentences, function(sentence, huliu.positive, huliu.negative)
	{
		sentence = gsub('[[:punct:]]',' ',sentence)
		sentence = gsub('[[:cntrl:]]','',sentence)
		sentence = gsub('\\d+','',sentence)  #removes decimal number
		sentence = gsub('\n','',sentence)    #removes new lines
		sentence = tolower(sentence)
		word.list = str_split(sentence, '\\s+')
		words = unlist(word.list)  #changes a list to character vector
		pos.matches = match(words, huliu.positive)
		neg.matches = match(words, huliu.negative)
		pos.matches = !is.na(pos.matches)
		neg.matches = !is.na(neg.matches)
		pp = sum(pos.matches)
		nn = sum(neg.matches)
		score = sum(pos.matches) - sum(neg.matches)
		list1 = c(score, pp, nn)
		return (list1)
	}, huliu.positive, huliu.negative)
	score_new = lapply(list, `[[`, 1)
	pp1 = lapply(list, `[[`, 2)
	nn1 = lapply(list, `[[`, 3)

	scores.df = data.frame(score = score_new, text=sentences)
	positive.df = data.frame(Positive = pp1, text=sentences)
	negative.df = data.frame(Negative = nn1, text=sentences)

	list_df = list(scores.df, positive.df, negative.df)
	return(list_df)
}

```

```{r loading twitter data}
setwd("C:/Users/User/Desktop/Ajax_Jian/StartCo_Seed_Hatchery-master")
huliu.positive <- scan("C:/Users/User/Desktop/Ajax_Jian/StartCo_Seed_Hatchery-master/positive-words.txt", what='character', comment.char=';')
huliu.negative <- scan("C:/Users/User/Desktop/Ajax_Jian/StartCo_Seed_Hatchery-master/negative-words.txt", what='character', comment.char=';')
huliu.negative <- c(huliu.negative,'wtf','wait','waiting','epicfail', 'crash', 'bug', 'bugy', 'bugs', 'slow', 'lie')
#cred$handshake(cainfo="cacert.pem")
URL_prod_rev <- 
  "https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents-financials.csv"
SP500 <- read.csv(URL_prod_rev, 
                    header=T, 
                    sep=",",
                    stringsAsFactors=FALSE)

SP500_tickers <- paste("$",SP500$Symbol, sep = "")
SP500_tickers <- paste(SP500_tickers,SP500$Name, sep = " ")

#loop that query all 500 stocks
tweets<- list()
for (i in SP500_tickers[1:3]) {
  tweets[i] <- list(searchTwitter(i,n=1500 ,lang = "en", since = '2017-03-14'))
}

#convert to data frame
tweets<- unlist(tweets)
df<- do.call("rbind", lapply(tweets,as.data.frame))
library(dplyr)
#retweet<- df[length(which(df$retweetCount>0 | df$favoriteCount >0)),]

df$text <- sapply(df$text, function(row) iconv(row, "latin1","ASCII",sub = ""))
df$text <- gsub("(f|ht)tp(s?)://(.*)[.][a-z]+","",df$text)
df$text = tolower(df$text)
  # remove rt
df$text = gsub("rt", "", df$text)
  # remove at
df$text = gsub("@\\w+", "", df$text)
  # remove punctuation
df$text = gsub("[[:punct:]]", "", df$text)
  # remove control characters
df$text <-  gsub('[[:cntrl:]]', '', df$text)
  # remove numbers
df$text = gsub("[[:digit:]]", "", df$text)
  # remove links http
df$text = gsub("http\\w+",  "", df$text)
  # remove tabs
df$text = gsub("[ |\t]{2,}", " ", df$text)
  # remove new lines
df$text = gsub("[ |\n]{1,}", " ", df$text)
  # remove blank spaces at the beginning
df$text = gsub("^ ", "", df$text)
  # remove blank spaces at the end
df$text = gsub(" $", "", df$text)

df<- add_rownames(df, "VALUE")
df$VALUE<-gsub('[[:digit:]]+', '', df$VALUE)

raw.sample <- df$text
head(raw.sample)
```

```{r lets play!}
result <- score.sentiment(raw.sample, huliu.positive, huliu.negative)

library(reshape)
#create a copy of the result
test1 = result[[1]] #score
test2 = result[[2]] #postive scores
test3 = result[[3]] #negative scores

test1$text <- NULL
test2$text <- NULL
test3$text <- NULL

q1 <- test1[1,]
q2 <- test2[1,]
q3 <- test3[1,]

qq1 <- melt(q1, , var= 'Score')
qq2 <- melt(q2, , var= 'Positive')
qq3 <- melt(q3, , var= 'Negative')

qq1['Score'] = NULL
qq2['Positive'] = NULL
qq3['Negative'] = NULL

table1 <- data.frame(Text=result[[1]]$text, Score=qq1)
table2 <- data.frame(Text=result[[2]]$text, Score=qq2)
table3 <- data.frame(Text=result[[3]]$text, Score=qq3)

#Merge the tables
table_final <- data.frame(Text = table1$Text, Score= table1$value, Positive = table2$value, Negative = table3$value)
#DISAG.Var = abs(1-abs((table_final$Positive-table_final$Negative)/(table_final$Positive+table_final$Negative)))
head(table_final)
rm(Sentiment_merge)
Sentiment_merge<- cbind(df,table_final)
```

```{r more news api}
install.packages("jsonlite")
library(jsonlite)
news <- as.data.frame(fromJSON("https://newsapi.org/v1/articles?source=the-wall-street-journal&sortBy=top&apiKey=53903acb258049688a6a6acecc569d42"))
NEWSAPI.org <- "53903acb258049688a6a6acecc569d42"
THEGUARDIAN <- "65ed54d8-3907-4875-ad23-28151efdf0aa"
#https://newsapi.org/v1/articles?source=the-wall-street-journal&sortBy=top&apiKey=53903acb258049688a6a6acecc569d42

news2<- as.data.frame(fromJSON("https://newsapi.org/v1/articles?source=the-next-web&sortBy=latest&apiKey=53903acb258049688a6a6acecc569d42"))

bloomberg <- as.data.frame(fromJSON("https://newsapi.org/v1/articles?source=bloomberg&sortBy=top&apiKey=53903acb258049688a6a6acecc569d42"))
```

`BAG OF WORDS` build two bags of words: one with words that associates with stock rise, one with stock price drop. Then use Bayesian Probability to classify them.

```{r}
#USING THE tm PACKAGE
library(tm)
text<-as.character(Sentiment_merge$Text)
head(text)
text = c("Doc1;","This is doc2 --", "And, then Doc3.")
ctext = Corpus(VectorSource(text))
ctext
#writeCorpus(ctext)
str(text)
#THE CORPUS IS A LIST OBJECT in R of type VCorpus or Corpus
inspect(ctext)

print(as.character(ctext[[1]]))

print(lapply(ctext[1:2],as.character))

ctext = tm_map(ctext,tolower)  #Lower case all text in all docs
inspect(ctext)

ctext2 = tm_map(ctext,toupper)
inspect(ctext2)

#dropping words
dropWords = c("IS","AND","THEN")
ctext2 = tm_map(ctext2,removeWords,dropWords)
inspect(ctext2)

temp = tm_map(ctext2,removeWords,stopwords("english"))
print(lapply(temp,as.character))
temp = tm_map(temp,removeNumbers)
print(lapply(temp,as.character))

#CONVERT CORPUS INTO ARRAY OF STRINGS AND FLATTEN
ctext = Corpus(VectorSource(text))
ctext

print(lapply(ctext, as.character))

txt = NULL
for (j in 1:length(ctext)) {
  txt = c(txt,ctext[[j]]$content)
}
txt = paste(txt,collapse=" ")
txt = tolower(txt)
print(txt)

#TERM-DOCUMENT MATRIX
tdm = TermDocumentMatrix(ctext,control=list(minWordLength=1))
print(tdm)

inspect(tdm[10:20,11:18])

out = findFreqTerms(tdm,lowfreq=5)
print(out)

library(tm)
textarray = c("Free software comes with ABSOLUTELY NO certain WARRANTY","You are welcome to redistribute free software under certain conditions","Natural language support for software in an English locale","A collaborative project with many contributors")
textcorpus = Corpus(VectorSource(textarray))
m = TermDocumentMatrix(textcorpus)
print(as.matrix(m))
print(as.matrix(weightTfIdf(m)))

#STEMMING
ctext2 = tm_map(ctext,removeWords,stopwords("english"))
ctext2 = tm_map(ctext2, stemDocument)
print(lapply(ctext2, as.character))
```

Note how closely this is related to the disagreement index seen earlier. 

- The bullishness index does not predict returns, but returns do explain message posting. More messages are posted in periods of negative returns, but this is not a significant relationship. 

- A contemporaneous relation between returns and bullishness is present. Overall, $AF04$ present some important results that are indicative of the results in this literature, confirmed also in subsequent work. 

- First, that message board postings do not predict returns. 

- Second, that disagreement (measured from postings) induces trading.

- Third, message posting does predict volatility at daily frequencies and intraday. 

- Fourth, messages reflect public information rapidly. Overall, they conclude that stock chat is meaningful in content and not just noise. 




## Possibile Applications for Finance Firms

An illustrative list of **applications** for finance firms is as follows:

- Monitoring corporate buzz.
- Analyzing textual data to detect, analyze, and understand the more profitable customers or products.
- Targeting new clients.
- Customer retention, which is a huge issue. Text mining complaints to prioritize customer remedial action makes a huge difference, especially in the insurance business. 
- Lending activity - automated management of profiling information for lending screening. 
- Market prediction and trading.
- Risk management.
- Automated financial analysts.
- Financial forensics to prevent rogue employees from inflicting large losses.
- Fraud detection.
- Detecting market manipulation.
- Social network analysis of clients.
- Measuring institutional risk from systemic risk.

## What is LSA?

Latent Semantic Analysis (LSA) is an approach for reducing the dimension of the Term-Document Matrix (TDM), or the corresponding Document-Term Matrix (DTM), in general used interchangeably, unless a specific one is invoked. Dimension reduction of the TDM offers two benefits:

- The DTM is usually a sparse matrix, and sparseness means that our algorithms have to work harder on missing data, which is clearly wasteful. Some of this sparseness is attenuated by applying LSA to the TDM. 

- The problem of synonymy also exists in the TDM, which usually contains thousands of terms (words). Synonymy arises becauses many words have similar meanings, i.e., redundancy exists in the list of terms. LSA mitigates this redundancy, as we shall see through the ensuing anaysis of LSA. 

- While not precisely the same thing, think of LSA in the text domain as analogous to PCA in the data domain. 

## How is LSA implemented using SVD?

LSA is the application of Singular Value Decomposition (SVD) to the TDM, extracted from a text corpus. Define the TDM to be a matrix $M \in {\cal R}^{m \times n}$, where $m$ is the number of terms and $n$ is the number of documents. 

```{r machine learning}
install.packages("RTextTools")
library(tm)
library(RTextTools)

#Create sample text with positive and negative markers
n = 1000
npos = round(runif(n,1,25))
nneg = round(runif(n,1,25))
flag = matrix(0,n,1)
flag[which(npos>nneg)] = 1
text = NULL
for (j in 1:n) {
  res = paste(c(sample(huliu.positive,npos[j]),sample(huliu.negative,nneg[j])),collapse=" ")
  text = c(text,res)
}

#Text Classification
m = create_matrix(text)
print(m)
m = create_matrix(text,weighting=weightTfIdf)
print(m)
container <- create_container(m,flag,trainSize=1:(n/2), testSize=(n/2+1):n,virgin=FALSE)
models <- train_models(container, algorithms=c("MAXENT","SVM","GLMNET","SLDA","TREE","BAGGING","BOOSTING","RF"))
models <- train_models(container, algorithms=c("MAXENT","SVM","GLMNET","TREE"))
results <- classify_models(container, models)
analytics <- create_analytics(container, results)

#RESULTS
analytics@algorithm_summary # SUMMARY OF PRECISION, RECALL, F-SCORES, AND ACCURACY SORTED BY TOPIC CODE FOR EACH ALGORITHM
analytics@label_summary # SUMMARY OF LABEL (e.g. TOPIC) ACCURACY
analytics@document_summary # RAW SUMMARY OF ALL DATA AND SCORING
analytics@ensemble_summary # SUMMARY OF ENSEMBLE PRECISION/COVERAGE. USES THE n VARIABLE PASSED INTO create_analytics()

#CONFUSION MATRIX
yhat = as.matrix(analytics@document_summary$CONSENSUS_CODE)
y = flag[(n/2+1):n]
print(table(y,yhat))

```

Create a temporary directory and add some documents to it. This is a modification of the example in the **lsa** package

```{r}
system("mkdir D")
write( c("blue", "red", "green"), file=paste("D", "D1.txt", sep="/"))
write( c("black", "blue", "red"), file=paste("D", "D2.txt", sep="/"))
write( c("yellow", "black", "green"), file=paste("D", "D3.txt", sep="/"))
write( c("yellow", "red", "black"), file=paste("D", "D4.txt", sep="/"))
```

Create a TDM using the **textmatrix** function.

```{r}
library(lsa)
tdm = textmatrix("D",minWordLength=1)
print(tdm)
```

Remove the extra directory. 
```{r}
system("rm -rf D")
```


## So, what does SVD do? 

SVD tries to connect the correlation matrix of terms ($M \cdot M^\top$) with the correlation matrix of documents ($M^\top \cdot M$) through the singular matrix. 

To see this connection, note that matrix $T$ contains the eigenvectors of the correlation matrix of terms. Likewise, the matrix $D$ contains the eigenvectors of the correlation matrix of documents. To see this, let's compute

```{r}
et = eigen(tdm %*% t(tdm))$vectors
print(et)
ed = eigen(t(tdm) %*% tdm)$vectors
print(ed)
```

## Dimension reduction of the TDM via LSA

If we wish to reduce the dimension of the latent semantic space to $k < n$ then we use only the first $k$ eigenvectors. The **lsa** function does this automatically. 

We call LSA and ask it to automatically reduce the dimension of the TDM using a built-in function **dimcalc_share**.

```{r}
res = lsa(tdm,dims=dimcalc_share())
print(res)
```

We can see that the dimension has been reduced from $n=4$ to $n=2$. The output is shown for both the term matrix and the document matrix, both of which have only two columns. Think of these as the two "principal semantic components" of the TDM. 

Compare the output of the LSA to the eigenvectors above to see that it is exactly that. The singular values in the ouput are connected to SVD as follows. 

## LSA and SVD: the connection?

First of all we see that the **lsa** function is nothing but the **svd** function in base R. 

```{r}
res2 = svd(tdm)
print(res2)
```

The output here is the same as that of LSA except it is provided for $n=4$. So we have four columns in $T$ and $D$ rather than two. Compare the results here to the previous two slides to see the connection. 

## What is the rank of the TDM? 

We may reconstruct the TDM using the result of the LSA. 

```{r}
tdm_lsa = res$tk %*% diag(res$sk) %*% t(res$dk)
print(tdm_lsa)
```

We see the new TDM after the LSA operation, it has non-integer frequency counts, but it may be treated in the same way as the original TDM. The document vectors populate a slightly different hyperspace. 

LSA reduces the rank of the correlation matrix of terms $M \cdot M^\top$ to $n=2$. Here we see the rank before and after LSA. 

```{r}
library(Matrix)
print(rankMatrix(tdm))
print(rankMatrix(tdm_lsa))
```

## And LDA, what does it have to do with LSA?

It is similar to LSA, in that it seeks to find the most related words and cluster them into topics. It uses a Bayesian approach to do this, but more on that later. Here, let's just do an example to see how we might use the **topicmodels** package. 

```{r}
#Load the package
library(topicmodels)

#Load data on news articles from Associated Press
data(AssociatedPress)
print(dim(AssociatedPress))
```

This is a large DTM (not TDM). It has more than 10,000 terms, and more than 2,000 documents. This is very large and LDA will take some time, so let's run it on a subset of the documents. 

```{r}
dtm = AssociatedPress[1:100,]
dim(dtm)
```


## Now we run LDA on this data set

```{r}
#Set parameters for Gibbs sampling
burnin = 4000
iter = 2000
thin = 500
seed = list(2003,5,63,100001,765)
nstart = 5
best = TRUE

#Number of topics
k = 5
```

```{r}
#Run LDA
res <-LDA(dtm, k, method="Gibbs", control = list(nstart = nstart, seed = seed, best = best, burnin = burnin, iter = iter, thin = thin))

#Show topics
res.topics = as.matrix(topics(res))
print(res.topics)

#Show top terms
res.terms = as.matrix(terms(res,10))
print(res.terms)

#Show topic probabilities
res.topicProbs = as.data.frame(res@gamma)
print(res.topicProbs)

#Check that each term is allocated to all topics
print(rowSums(res.topicProbs))
```
